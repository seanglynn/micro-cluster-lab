{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure to link pyspark to the right Spark folder with findspark\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"pysparkYieldData\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-09-01 19:11 hdfs://node-master:9000/user/root\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls hdfs://node-master:9000/user/$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bash-Interface.ipynb  PySparkYieldData.ipynb  dask-worker-space\r\n",
      "Dask-Yarn.ipynb       Python-Spark.ipynb      datasets\r\n"
     ]
    }
   ],
   "source": [
    "! ls $PWD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -put $PWD/datasets/input_data.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GZ File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see it as part of the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls hdfs://node-master:9000/user/$USER/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls hdfs://node-master:9000/user/root/input_data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file = sc.textFile(\"input_data.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = parse_file.map(lambda l: l.split(\"\\t\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "user_activity = parts.map(lambda p: Row(\n",
    "    date=p[0], \n",
    "    time=(p[1]),\n",
    "    user_id=(p[2]),\n",
    "    url=(p[3]),\n",
    "    ip=(p[4]),\n",
    "    user_agent_str=(p[5]),\n",
    "\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaUsers = spark.createDataFrame(user_activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[date: string, time: string, user_id: string, url: string, ip: string, user_agent_str: string]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaUsers.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      date|    time|             user_id|                 url|                  ip|      user_agent_str|\n",
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2014-10-12|17:01:01|f4fdd9e55192e9475...|http://6f2a9cab64...|       94.11.238.152|Mozilla/5.0 (iPad...|\n",
      "|2014-10-12|17:01:01|0ae53126499336757...|http://8eb4ac417c...|       92.238.71.109|Mozilla/5.0 (iPad...|\n",
      "|2014-10-12|17:01:01|c5ac174ee153f7e57...|https://1415d3778...|         2.26.44.196|Mozilla/5.0 (Linu...|\n",
      "|2014-10-12|17:01:01|2d86766f9908fde41...|http://47e1f0cca5...|194.81.33.57, 66....|Mozilla/5.0 (Linu...|\n",
      "|2014-10-12|17:01:01|3938fffe5c0a131f5...|https://978c17aed...|      109.152.120.12|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|88eb65d5f952f3bf5...|http://38d6db9ae3...|         2.28.82.212|Mozilla/5.0 (iPad...|\n",
      "|2014-10-12|17:01:01|068d17d3e73ea7aac...|http://6b2716ce0f...|      86.173.122.128|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|068d17d3e73ea7aac...|http://6b2716ce0f...|      86.173.122.128|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|fff34e499a2c4c287...|http://38d6db9ae3...|          5.67.165.5|Mozilla/5.0 (iPad...|\n",
      "|2014-10-12|17:01:01|fff34e499a2c4c287...|http://38d6db9ae3...|          5.67.165.5|Mozilla/5.0 (iPad...|\n",
      "|2014-10-12|17:01:01|23c2108f8f8455e3d...|http://8164e3b926...|      77.103.144.147|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|a43991585c5bef203...|http://38d6db9ae3...|       92.233.50.166|Mozilla/5.0 (Linu...|\n",
      "|2014-10-12|17:01:01|5142886fef08b33f0...|http://38d6db9ae3...|       92.233.50.166|Mozilla/5.0 (Linu...|\n",
      "|2014-10-12|17:01:01|97b39ada1fb010951...|http://8c25ee1097...|      78.144.141.170|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|2597ae0aefd1526a7...|http://b9d6e86635...|        88.110.78.16|Mozilla/5.0 (iPho...|\n",
      "|2014-10-12|17:01:01|579f4f34854446ac2...|https://9539ccbfe...|     191.235.210.153|Mozilla/5.0 (X11;...|\n",
      "|2014-10-12|17:01:01|97b39ada1fb010951...|http://8c25ee1097...|      78.144.141.170|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|97b39ada1fb010951...|http://8c25ee1097...|      78.144.141.170|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|97b39ada1fb010951...|http://8c25ee1097...|      78.144.141.170|Mozilla/5.0 (Wind...|\n",
      "|2014-10-12|17:01:01|ab4c808716db9a7e0...|http://6c9083b2c9...|        46.16.73.192|Mozilla/5.0 (Unkn...|\n",
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaUsers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retrieve geostats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import requests\n",
    "api_key=\"7c6407327a9eaf157578e80dfea828e9\"\n",
    "\n",
    "# @udf\n",
    "def get_country_from_ip(ip_address):\n",
    "    url= f\"http://api.ipapi.com/{ip_address}?access_key={api_key}\"\n",
    "    r = requests.get(url)\n",
    "    ip_response_json=r.json()\n",
    "\n",
    "    country=ip_response_json['country_name']\n",
    "    city=ip_response_json['city']\n",
    "    return (country, city)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_ip_addresses.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24820"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "unique_ip_addresses=schemaUsers.withColumn('ip', split(schemaUsers['ip'], ', ')[0]).select(\"ip\").distinct()\n",
    "\n",
    "unique_addresses_formatted = unique_ip_addresses.select(unique_ip_addresses.columns[0])\n",
    "# unique_ip_addresses=schemaUsers.select(\"ip\").distinct().withColumn('ip', split(schemaUsers['ip'], ', '))\n",
    "\n",
    "unique_addresses.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_addresses_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_addresses_formatted.write.format(\"text\").save(\"ip_addresses.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 4 items\n",
      "drwxr-xr-x   - root supergroup          0 2021-09-01 19:36 hdfs://node-master:9000/user/root/.skein\n",
      "drwxr-xr-x   - root supergroup          0 2021-09-01 19:36 hdfs://node-master:9000/user/root/.sparkStaging\n",
      "-rw-r--r--   2 root supergroup    7866234 2021-09-01 17:15 hdfs://node-master:9000/user/root/input_data.gz\n",
      "drwxr-xr-x   - root supergroup          0 2021-09-01 21:36 hdfs://node-master:9000/user/root/ip_addresses.txt\n"
     ]
    }
   ],
   "source": [
    "! hadoop dfs -ls hdfs://node-master:9000/user/$USER/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.2.4 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)\r\n"
     ]
    }
   ],
   "source": [
    "! pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-01 21:50:02--  https://download.maxmind.com/app/geoip_download_by_token?edition_id=GeoLite2-Country-CSV&date=20210831&suffix=zip&token=v2.local.zkJeIzHOqnNOtHqMT8hnpq_kxp5D8Rw3SDCRxlnkDbg8z3uzOtjxtLZqsqgR1OV9A1QfnFNWixb1UQtbZzYx9Kbcfylx8WbOWhmjlLQFIS4Eq_BrsRDI1kH766K9a0I40B9wSgmft_YUwxnJjCfS1jgPqGn9lBYDLXTJg_wYzTh2lSsEbIswtU19Al8XQN2zjVf0OQ\n",
      "Resolving download.maxmind.com (download.maxmind.com)... 104.16.38.47, 104.16.37.47, 2606:4700::6810:262f, ...\n",
      "Connecting to download.maxmind.com (download.maxmind.com)|104.16.38.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3586289 (3.4M) [application/zip]\n",
      "Saving to: '/tmp/geo_country.csv'\n",
      "\n",
      "/tmp/geo_country.cs 100%[===================>]   3.42M  12.0MB/s    in 0.3s    \n",
      "\n",
      "2021-09-01 21:50:03 (12.0 MB/s) - '/tmp/geo_country.csv' saved [3586289/3586289]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! export URL_1=\"https://download.maxmind.com/app/geoip_download_by_token?edition_id=GeoLite2-Country-CSV&date=20210831&suffix=zip&token=v2.local.zkJeIzHOqnNOtHqMT8hnpq_kxp5D8Rw3SDCRxlnkDbg8z3uzOtjxtLZqsqgR1OV9A1QfnFNWixb1UQtbZzYx9Kbcfylx8WbOWhmjlLQFIS4Eq_BrsRDI1kH766K9a0I40B9wSgmft_YUwxnJjCfS1jgPqGn9lBYDLXTJg_wYzTh2lSsEbIswtU19Al8XQN2zjVf0OQ\" ; wget $URL_1 -O /tmp/geo_country.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-01 21:51:09--  https://download.maxmind.com/app/geoip_download_by_token?edition_id=GeoLite2-City-CSV&date=20210831&suffix=zip&token=v2.local.11khj64wvdOZw1QcovhkAa4V2Nf4NHq7CG5TFWV7BKQ9vYRjJnhDxCL_TKpyYc9g_Yv9ZxnwYeH5hj920zkA-rabTt3wPN5Y8ebwiaJPJSNQgK0L1yYiKVvDWQw1VYW_EitmtCfwRseY52LjvFWZec4xIX_K-OUMZvB5H-nSIyiNF_38PgTrReDtF75GIGEP4ywjpw\n",
      "Resolving download.maxmind.com (download.maxmind.com)... 104.16.37.47, 104.16.38.47, 2606:4700::6810:262f, ...\n",
      "Connecting to download.maxmind.com (download.maxmind.com)|104.16.37.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50885905 (49M) [application/zip]\n",
      "Saving to: '/tmp/geo_city.csv'\n",
      "\n",
      "/tmp/geo_city.csv   100%[===================>]  48.53M  35.2MB/s    in 1.4s    \n",
      "\n",
      "2021-09-01 21:51:12 (35.2 MB/s) - '/tmp/geo_city.csv' saved [50885905/50885905]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! export URL_2=\"https://download.maxmind.com/app/geoip_download_by_token?edition_id=GeoLite2-City-CSV&date=20210831&suffix=zip&token=v2.local.11khj64wvdOZw1QcovhkAa4V2Nf4NHq7CG5TFWV7BKQ9vYRjJnhDxCL_TKpyYc9g_Yv9ZxnwYeH5hj920zkA-rabTt3wPN5Y8ebwiaJPJSNQgK0L1yYiKVvDWQw1VYW_EitmtCfwRseY52LjvFWZec4xIX_K-OUMZvB5H-nSIyiNF_38PgTrReDtF75GIGEP4ywjpw\" ; wget $URL_2 -O /tmp/geo_city.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -put /tmp/*.csv /user/root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   2 root supergroup   50885905 2021-09-01 21:53 /user/root/geo_city.csv\r\n",
      "-rw-r--r--   2 root supergroup    3586289 2021-09-01 21:53 /user/root/geo_country.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/*csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twighlight Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionRequiredError",
     "evalue": "You do not have permission to use this service interface.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPermissionRequiredError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-9ba28cd35229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y10N1oIl5QdTC7ot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgeoip2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'203.0.113.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/geoip2/webservice.py\u001b[0m in \u001b[0;36mcity\u001b[0;34m(self, ip_address)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeoip2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_address\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIPAddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"me\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCountry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/geoip2/webservice.py\u001b[0m in \u001b[0;36m_response_for\u001b[0;34m(self, path, model_class, ip_address)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_for_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0mdecoded_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_success\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_locales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionRequiredError\u001b[0m: You do not have permission to use this service interface."
     ]
    }
   ],
   "source": [
    "import geoip2.webservice\n",
    "\n",
    "# This reader object should be reused across lookups as creation of it is\n",
    "# expensive.\n",
    "accid=\"yurty\"\n",
    "key=\"yurt\"\n",
    "with geoip2.webservice.Client(accid, key) as client:\n",
    "    response = client.city('203.0.113.0')\n",
    "    print(response.country.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch maxmind-database.mmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geoip2\n",
      "  Downloading geoip2-4.2.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |################################| 62 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<2.0.0,>=1.25.2\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |################################| 138 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting maxminddb<3.0.0,>=2.0.0\n",
      "  Downloading maxminddb-2.0.3.tar.gz (286 kB)\n",
      "\u001b[K     |################################| 286 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.6.2 in /usr/local/lib/python3.6/dist-packages (from geoip2) (3.7.4.post0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (3.10.0.2)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (5.1.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0->aiohttp<4.0.0,>=3.6.2->geoip2) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (2018.1.18)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: maxminddb\n",
      "  Building wheel for maxminddb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for maxminddb: filename=maxminddb-2.0.3-py2.py3-none-any.whl size=13627 sha256=15af01ba9cf595a6502c27ede61e7427c84e2b36beb97e77f7177eeb2504e959\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/2a/dc/6297056132f19f12f369635b69417c3e2619cd08ec070b5e6c\n",
      "Successfully built maxminddb\n",
      "Installing collected packages: urllib3, charset-normalizer, requests, maxminddb, geoip2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "Successfully installed charset-normalizer-2.0.4 geoip2-4.2.0 maxminddb-2.0.3 requests-2.26.0 urllib3-1.26.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install geoip2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expensive! But hey.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'country_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-0ce84491ad4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mip_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_country_from_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-9a111e90b563>\u001b[0m in \u001b[0;36mget_country_from_ip\u001b[0;34m(ip_address)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mip_response_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mip_response_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mip_response_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'city'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'country_name'"
     ]
    }
   ],
   "source": [
    "# unique_ip_addresses.rdd.map(lambda row: row.asDict())\n",
    "\n",
    "\n",
    "ip_array = [str(row.ip) for row in unique_ip_addresses.collect()]\n",
    "\n",
    "ip_list=[]\n",
    "for ip in ip_array:\n",
    "    if ', ' in ip:\n",
    "        split_ip=ip.split(', ')\n",
    "        ip_list.append({ip: get_country_from_ip(split_ip[0])})\n",
    "        ip_list.append({ip: get_country_from_ip(split_ip[1])})\n",
    "\n",
    "    else:\n",
    "        ip_list.append({ip: get_country_from_ip(ip)})\n",
    "        \n",
    "len(ip_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failed on hitting limit API License for 24 k requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper but time consuming.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique IP Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_from_ip = udf(get_country_from_ip, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer says no..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9111c4df89ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_ip_addresses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_country_from_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n"
     ]
    }
   ],
   "source": [
    "unique_ip_addresses.select(*[get_country_from_ip('ip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_geo_df = unique_ip_addresses.select(get_country_from_ip(\"ip\").alias(\"geolocation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: Bad substitution\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-972440f4c3c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mip_geo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1630516469872_0005/container_1630516469872_0005_01_000002/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n"
     ]
    }
   ],
   "source": [
    "ip_geo_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ip in unique_ip_addresses_list:\n",
    "#     print('{}: {}'.format(type(ip), ip))\n",
    "\n",
    "dict(unique_ip_addresses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# api_key=\"\"\n",
    "ip_address=\"188.141.30.136\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ip_details(ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
